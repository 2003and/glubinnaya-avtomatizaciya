{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo list:\n",
    "- ~~book reading function~~\n",
    "- morphologic analysis:\n",
    "    - ~~lexical diversity calculation function~~\n",
    "    - ~~parts of speech counting function (by word's initial form)~~\n",
    "    - ~~parts of speech counting function (by tags, part of speech types)~~\n",
    "    - ~~text dynamics calculation functiion~~\n",
    "    - *top-1 for every speech part\n",
    "- syntax analysis\n",
    "    - reduce the amount of text\n",
    "    - ~~most popular root word~~\n",
    "- semantic analysis\n",
    "    - stop-word cleanup\n",
    "    - *automatic tag generation\n",
    "- graphic demonstration\n",
    "    - tag cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/andrew/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pip install -q nltk pymorphy2\n",
    "import nltk\n",
    "import pymorphy2\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load('ru_core_news_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a few constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT = ('.',',',':',';','\\'','\"','-','(',')','!','?','...','$','№')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preproc:\n",
    "    def __init__(self, book_name):\n",
    "        self.text = self.get_book_text(book_name)\n",
    "        self.tokens = nltk.word_tokenize(self.text)\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        self.tokens_no_punct = nltk.tokenize.RegexpTokenizer(r\"\\w+\").tokenize(self.text)\n",
    "        self.nlp = spacy.load('ru_core_news_sm')\n",
    "        self.doc = nlp(self.text)\n",
    "        self.lex_diversity()\n",
    "        self.make_tags()\n",
    "    \n",
    "    def process_stopwords(self, overwrite=False):\n",
    "        stopwords = stopwords.words(\"russian\")\n",
    "        if overwrite:\n",
    "            pass\n",
    "        else:\n",
    "            self.tokens_no_stopwords = [token for token in self.tokens if token not in stopwords]\n",
    "\n",
    "    def get_book_text(self, name):\n",
    "        with open(name, \"r\") as raw:\n",
    "            text = \"\"\n",
    "            for t in raw.readlines():\n",
    "                text += t+\"\\n\"\n",
    "            return text\n",
    "    \n",
    "    def lex_diversity(self):\n",
    "        unique_words_set = set(self.tokens)\n",
    "        self.lex_diversity_coeff = len(unique_words_set) / len(self.tokens)\n",
    "\n",
    "    def make_tags(self):\n",
    "        self.tagged = []\n",
    "        for token in self.tokens_no_punct:\n",
    "            self.tagged.append((token, self.morph.parse(token)[0].tag.POS))\n",
    "            # spacy find all собственные имена\n",
    "        return self.tagged\n",
    "\n",
    "    def text_dynamics(total_tags: list):\n",
    "        verbs = total_tags.count(\"VERB\")\n",
    "        return verbs / len(total_tags)\n",
    "\n",
    "    def text_dynamics_dict(self):\n",
    "        self.count_speech_parts()\n",
    "        verbs = self.tags_count[\"VERB\"]\n",
    "        total = 0\n",
    "        for i in self.tags_count.keys():\n",
    "            total += self.tags_count[i]\n",
    "        return verbs/total\n",
    "    \n",
    "    def count_speech_parts(self):\n",
    "        self.make_tags()\n",
    "        res = dict()\n",
    "        for token in self.tagged:\n",
    "            if (token[1] in res.keys()) :\n",
    "                res[token[1]] += 1\n",
    "            else:\n",
    "                res[token[1]] = 1\n",
    "        self.tags_count = res\n",
    "\n",
    "    def count_unique_words(self):\n",
    "        self.make_tags()\n",
    "        res = dict()\n",
    "        for token in self.tagged:\n",
    "            if not (token[1] in (\"CONJ\", \"PREP\", \"PRCL\")):\n",
    "                if token[0] in res.keys():\n",
    "                    res[token[0]] += 1\n",
    "                else:\n",
    "                    res[token[0]] = 1\n",
    "        self.word_count = res\n",
    "        self.word_count_sorted = dict(sorted(self.word_count.items(), key=lambda item: item[1]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_books(self, book1, book2):\n",
    "        text1 = self.get_book_text(book1)\n",
    "        text2 = self.get_book_text(book2)\n",
    "\n",
    "        doc1 = self.nlp(text1)\n",
    "        doc2 = self.nlp(text2)\n",
    "        return (doc1.similarity(doc2), np.dot(doc1.vector, doc2.vector) / (np.linalg.norm(doc1.vector) * (np.linalg.norm(doc2.vector))))\n",
    "\n",
    "    def count_root_speech_parts(self):\n",
    "        res = dict()\n",
    "        for token in self.doc:\n",
    "            if token.pos_ not in (\"SPACE\", \"PUNCT\"):\n",
    "                if token.pos_ not in res.keys():\n",
    "                    res[token.pos_] = 1\n",
    "                else:\n",
    "                    res[token.pos_] += 1\n",
    "        self.root_count = res\n",
    "        self.root_count_sorted = dict(sorted(self.root_count.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text file reading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book(name):\n",
    "    with open(name, \"r\") as raw:\n",
    "        text = \"\"\n",
    "        for t in raw.readlines():\n",
    "            text += t+\"\\n\"\n",
    "        return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical diversity calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_diversity(word_list):\n",
    "    unique_words_set = set(word_list)\n",
    "    return len(unique_words_set) / len(word_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text dynamics calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_dynamics(total_tags: list):\n",
    "    verbs = total_tags.count(\"VERB\")\n",
    "    return verbs / len(total_tags)\n",
    "\n",
    "def text_dynamics_dict(tags_dict):\n",
    "    verbs = tags_dict[\"VERB\"]\n",
    "    total = 0\n",
    "    for i in tags_dict.keys():\n",
    "        total += tags_dict[i]\n",
    "    return verbs/total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morph analyzer init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def tagger(tokenized):\n",
    "    res = []\n",
    "    for token in tokenized:\n",
    "        res.append((token, morph.parse(token)[0].tag.POS))\n",
    "        # spacy find all собственные имена\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of speech counting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter to count speech parts\n",
    "def count_speech_parts(tagged):\n",
    "    res = dict()\n",
    "    for token in tagged:\n",
    "        if (token[1] in res.keys()) :\n",
    "            res[token[1]] += 1\n",
    "        else:\n",
    "            res[token[1]] = 1\n",
    "    return res\n",
    "\n",
    "def count_unique_words(tagged):\n",
    "    res = dict()\n",
    "    for token in tagged:\n",
    "        if not (token[1] in (\"CONJ\", \"PREP\", \"PRCL\")):\n",
    "            if token[0] in res.keys():\n",
    "                res[token[0]] += 1\n",
    "            else:\n",
    "                res[token[0]] = 1\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax analyzing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_root_speech_parts(doc):\n",
    "    res = dict()\n",
    "    for token in doc:\n",
    "        if token.pos_ not in (\"SPACE\", \"PUNCT\"):\n",
    "            if token.pos_ not in res.keys():\n",
    "                res[token.pos_] = 1\n",
    "            else:\n",
    "                res[token.pos_] += 1\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now onto the fun part\n",
    "# Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13607/1686159224.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlist_of_books\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Похождения Чичикова (1921)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Белая гвардия (1922)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Роковые яйца (1924)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Собачье сердце (1925)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Мастер и маргарита (1929)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Театральный роман (1936)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_books\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpreproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Lexical diversity: {str(preproc.lex_diversity_coeff)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13607/2104827763.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, book_name)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_no_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\w+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ru_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlex_diversity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \"\"\"\n\u001b[0;32m-> 1037\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m_ensure_doc\u001b[0;34m(self, doc_like)\u001b[0m\n\u001b[1;32m   1126\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_like\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m             )\n\u001b[0;32m-> 1120\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_ensure_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_like\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._apply_special_cases\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._prepare_special_spans\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/tokens/doc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mnormalize_slice\u001b[0;34m(length, start, stop, step)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1264\u001b[0;31m def normalize_slice(\n\u001b[0m\u001b[1;32m   1265\u001b[0m     \u001b[0mlength\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m ) -> Tuple[int, int]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_of_books = [\"Похождения Чичикова (1921)\",\"Белая гвардия (1922)\",\"Роковые яйца (1924)\",\"Собачье сердце (1925)\",\"Мастер и маргарита (1929)\",\"Театральный роман (1936)\"]\n",
    "for name in list_of_books:\n",
    "    preproc = Preproc(name)\n",
    "    with open(name+\"_out\", \"w\") as file:\n",
    "        preproc.count_unique_words()\n",
    "        preproc.count_speech_parts()\n",
    "        preproc.count_root_speech_parts()\n",
    "        \n",
    "        file.write(f\"Lexical diversity: {str(preproc.lex_diversity_coeff)}\\n\")\n",
    "\n",
    "        file.write(\"Top-3 words: \\n\")\n",
    "        file.write(f\"\\t{list(preproc.word_count_sorted.keys())[-1]}\\n\")\n",
    "        file.write(f\"\\t{list(preproc.word_count_sorted.keys())[-2]}\\n\")\n",
    "        file.write(f\"\\t{list(preproc.word_count_sorted.keys())[-3]}\\n\")\n",
    "\n",
    "        file.write(f\"Text dynamics: {preproc.text_dynamics_dict()}\\n\")\n",
    "\n",
    "        file.write(f\"Top root speech part: {list(preproc.root_count_sorted.keys())[-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
