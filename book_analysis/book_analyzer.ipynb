{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo list:\n",
    "- ~~book reading function~~\n",
    "- morphologic analysis:\n",
    "    - ~~lexical diversity calculation function~~\n",
    "    - ~~parts of speech counting function (by word's initial form)~~\n",
    "    - ~~parts of speech counting function (by tags, part of speech types)~~\n",
    "    - ~~text dynamics calculation functiion~~\n",
    "    - *top-1 for every speech part\n",
    "- syntax analysis\n",
    "    - reduce the amount of text\n",
    "    - ~~most popular root word~~\n",
    "- semantic analysis\n",
    "    - ~~stop-word cleanup~~\n",
    "    - *automatic tag generation\n",
    "- graphic demonstration\n",
    "    - tag cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/andrew/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pip install -q nltk pymorphy2 gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "import spacy\n",
    "import numpy as np\n",
    "import gensim.downloader\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "# word2vec_rus = gensim.downloader.load('word2vec-ruscorpora-300')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a few constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT = ('.',',',':',';','\\'','\"','-','(',')','!','?','...','$','№')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preproc:\n",
    "    def __init__(self, book_name):\n",
    "        self.text = self.get_book_text(book_name)\n",
    "        self.tokens = nltk.word_tokenize(self.text)\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        self.tokens_no_punct = nltk.tokenize.RegexpTokenizer(r\"\\w+\").tokenize(self.text)\n",
    "        self.nlp = spacy.load('ru_core_news_sm')\n",
    "        self.word2vec_rus = gensim.downloader.load('word2vec-ruscorpora-300')\n",
    "        self.doc = nlp(self.text)\n",
    "        self.lemm = nltk.WordNetLemmatizer()\n",
    "        self.lex_diversity()\n",
    "        self.make_tags()\n",
    "    \n",
    "    def process_stopwords(self, overwrite=False):\n",
    "        stopw = stopwords.words(\"russian\")\n",
    "        if overwrite:\n",
    "            self.tokens = [token for token in self.tokens if token not in stopw]\n",
    "        else:\n",
    "            self.tokens_no_stopwords = [token for token in self.tokens if token not in stopw]\n",
    "\n",
    "    def get_book_text(self, name):\n",
    "        with open(name, \"r\") as raw:\n",
    "            text = \"\"\n",
    "            for t in raw.readlines():\n",
    "                text += t+\"\\n\"\n",
    "            return text\n",
    "    \n",
    "    def lex_diversity(self):\n",
    "        unique_words_set = set(self.tokens)\n",
    "        self.lex_diversity_coeff = len(unique_words_set) / len(self.tokens)\n",
    "\n",
    "    def make_tags(self):\n",
    "        self.tagged = []\n",
    "        self.tokens_and_tags = []\n",
    "        for token in self.tokens_no_punct:\n",
    "            tag = self.morph.parse(token)[0].tag.POS\n",
    "            self.tagged.append((token, tag))\n",
    "            # self.tokens_and_tags.append(self.lemm.lemmatize(token).lower()+'_'+tag)\n",
    "            # spacy find all собственные имена\n",
    "        return self.tagged\n",
    "\n",
    "    def text_dynamics(total_tags: list):\n",
    "        verbs = total_tags.count(\"VERB\")\n",
    "        return verbs / len(total_tags)\n",
    "\n",
    "    def text_dynamics_dict(self):\n",
    "        self.count_speech_parts()\n",
    "        verbs = self.tags_count[\"VERB\"]\n",
    "        total = 0\n",
    "        for i in self.tags_count.keys():\n",
    "            total += self.tags_count[i]\n",
    "        return verbs/total\n",
    "    \n",
    "    def count_speech_parts(self):\n",
    "        self.make_tags()\n",
    "        res = dict()\n",
    "        for token in self.tagged:\n",
    "            if (token[1] in res.keys()) :\n",
    "                res[token[1]] += 1\n",
    "            else:\n",
    "                res[token[1]] = 1\n",
    "        self.tags_count = res\n",
    "\n",
    "    def count_unique_words(self):\n",
    "        self.make_tags()\n",
    "        res = dict()\n",
    "        for token in self.tagged:\n",
    "            if not (token[1] in (\"CONJ\", \"PREP\", \"PRCL\")):\n",
    "                if token[0] in res.keys():\n",
    "                    res[token[0]] += 1\n",
    "                else:\n",
    "                    res[token[0]] = 1\n",
    "        self.word_count = res\n",
    "        self.word_count_sorted = dict(sorted(self.word_count.items(), key=lambda item: item[1]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_books(self, book1, book2):\n",
    "        text1 = self.get_book_text(book1)\n",
    "        text2 = self.get_book_text(book2)\n",
    "\n",
    "        doc1 = self.nlp(text1)\n",
    "        doc2 = self.nlp(text2)\n",
    "        return (doc1.similarity(doc2), np.dot(doc1.vector, doc2.vector) / (np.linalg.norm(doc1.vector) * (np.linalg.norm(doc2.vector))))\n",
    "\n",
    "    def count_root_speech_parts(self):\n",
    "        res = dict()\n",
    "        for token in self.doc:\n",
    "            if token.pos_ not in (\"SPACE\", \"PUNCT\"):\n",
    "                if token.pos_ not in res.keys():\n",
    "                    res[token.pos_] = 1\n",
    "                else:\n",
    "                    res[token.pos_] += 1\n",
    "        self.root_count = res\n",
    "        self.root_count_sorted = dict(sorted(self.root_count.items(), key=lambda item: item[1]))\n",
    "    \n",
    "    def fetch_tokens(self, amount=100, index=0):\n",
    "        return [self.tokens[token] for token in range(index, amount)]\n",
    "\n",
    "    def get_tokens_and_tags(self, amount=100):\n",
    "        res = []\n",
    "        i = 0\n",
    "        while len(res) < amount and i < len(self.doc):\n",
    "        # for i in range(min(amount, len(self.doc))):\n",
    "            if self.doc[i].tag_ not in (\"PUNCT\", \"SPACE\", \"PROPN\", \"ADP\", \"NUM\", \"CCONJ\", \"PART\", \"PRON\") and self.doc[i].lemma_ != \"-\":\n",
    "                res.append((self.doc[i].lemma_, self.doc[i].tag_))\n",
    "            i += 1\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def check_vectors(self, first_n_elements=100, logging=False):\n",
    "        res = []\n",
    "        i = 0\n",
    "        while len(res) < first_n_elements:\n",
    "            try:\n",
    "                res.append(self.word2vec_rus[self.doc[i].lemma_+'_'+self.tagged[i][1]]) # word2vec_rus[\"тест_NOUN\"]\n",
    "            except:\n",
    "                if logging:\n",
    "                    print(f\"error parsing {self.doc[i].lemma_+'_'+self.tagged[i][1]}\")\n",
    "                # res.append(self.doc[i].lemma_)\n",
    "            i += 1\n",
    "        return res\n",
    "    \n",
    "    def metamorph(self, first_n_elements=100):\n",
    "        res = \"\"\n",
    "        for i in self.check_vectors(first_n_elements):\n",
    "            res += self.word2vec_rus.most_similar(positive=[\"женщина_NOUN\", i], negative=\"мужчина_NOUN\")[-1][0].split(\"_\")[0] + \" \"\n",
    "        return res\n",
    "    \n",
    "    # def antonimize(word_list):\n",
    "    #     antonyms = []\n",
    "    #     for word in word_list:\n",
    "    #         synsets = wordnet.synsets(word)\n",
    "    #         for synset in synsets:\n",
    "    #             for lemma in synset.lemmas():\n",
    "    #                 if lemma.antonyms():\n",
    "    #                     antonyms.append(lemma.antonyms()[0].name())\n",
    "    #                     break\n",
    "    #     return antonyms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now onto the fun part\n",
    "# Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_58405/3509443336.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlist_of_books\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Похождения Чичикова (1921)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Белая гвардия (1922)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Роковые яйца (1924)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Собачье сердце (1925)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Мастер и маргарита (1929)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Театральный роман (1936)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_books\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpreproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpreproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_58405/2001562569.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, book_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpymorphy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMorphAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_no_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\w+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ru_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec_rus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec-ruscorpora-300'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"blank:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \"\"\"\n\u001b[1;32m    500\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ru_core_news_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m     return load_model_from_path(\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     )\n\u001b[0;32m--> 547\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[1;32m   2182\u001b[0m             \u001b[0;31m# Convert to list here in case exclude is (default) tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_link_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;31m# Split to support file names like meta.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m             \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mdeserialize_vocab\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdeserialize_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.from_disk\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/vectors.pyx\u001b[0m in \u001b[0;36mspacy.vectors.Vectors.from_disk\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m def from_disk(\n\u001b[0m\u001b[1;32m   1364\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m     \u001b[0mreaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_of_books = [\"Похождения Чичикова (1921)\",\"Белая гвардия (1922)\",\"Роковые яйца (1924)\",\"Собачье сердце (1925)\",\"Мастер и маргарита (1929)\",\"Театральный роман (1936)\"]\n",
    "for name in list_of_books:\n",
    "    preproc = Preproc(name)\n",
    "    with open(name+\"_out\", \"w\") as file:\n",
    "        preproc.process_stopwords()\n",
    "        preproc.count_unique_words()\n",
    "        preproc.count_speech_parts()\n",
    "        preproc.count_root_speech_parts()\n",
    "        # if name == list_of_books[0]:\n",
    "        print(f\"Metamorphed \\\"{name}\\\":\\n\",preproc.metamorph(), \"\\n\")\n",
    "        \n",
    "        file.write(f\"Lexical diversity: {str(preproc.lex_diversity_coeff)}\\n\")\n",
    "\n",
    "        file.write(\"Top-3 words: \\n\")\n",
    "        file.write(f\"\\t{list(preproc.word_count_sorted.keys())[-1]}\\n\")\n",
    "        file.write(f\"\\t{list(preproc.word_count_sorted.keys())[-2]}\\n\")\n",
    "        file.write(f\"\\t{list(preproc.word_count_sorted.keys())[-3]}\\n\")\n",
    "\n",
    "        file.write(f\"Text dynamics: {preproc.text_dynamics_dict()}\\n\")\n",
    "\n",
    "        file.write(f\"Top root speech part: {list(preproc.root_count_sorted.keys())[-1]}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual word2vec words processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = Preproc(list_of_books[4])\n",
    "preproc.process_stopwords(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tokens_and_tags = preproc.get_tokens_and_tags(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('стравинского', 'NOUN'),\n",
       " ('тот', 'DET'),\n",
       " ('время', 'NOUN'),\n",
       " ('как', 'SCONJ'),\n",
       " ('раз', 'NOUN'),\n",
       " ('как', 'ADV'),\n",
       " ('вести', 'VERB'),\n",
       " ('бездомный', 'ADJ'),\n",
       " ('долгий', 'ADJ'),\n",
       " ('сон', 'NOUN'),\n",
       " ('открыть', 'VERB'),\n",
       " ('глаз', 'NOUN'),\n",
       " ('некоторый', 'DET'),\n",
       " ('время', 'NOUN'),\n",
       " ('соображать', 'VERB'),\n",
       " ('как', 'SCONJ'),\n",
       " ('попасть', 'VERB'),\n",
       " ('этот', 'DET'),\n",
       " ('необыкновенный', 'ADJ'),\n",
       " ('комната', 'NOUN'),\n",
       " ('чистый', 'ADJ'),\n",
       " ('белый', 'ADJ'),\n",
       " ('стена', 'NOUN'),\n",
       " ('удивительный', 'ADJ'),\n",
       " ('ночной', 'ADJ'),\n",
       " ('столик', 'NOUN'),\n",
       " ('сделать', 'VERB'),\n",
       " ('какой', 'DET'),\n",
       " ('то', 'DET'),\n",
       " ('неизвестный', 'ADJ'),\n",
       " ('светлый', 'ADJ'),\n",
       " ('металл', 'NOUN'),\n",
       " ('величественной', 'VERB'),\n",
       " ('белый', 'ADJ'),\n",
       " ('штора', 'NOUN'),\n",
       " ('весь', 'DET'),\n",
       " ('стена', 'NOUN'),\n",
       " ('тряхнуть', 'VERB'),\n",
       " ('голова', 'NOUN'),\n",
       " ('убедиться', 'VERB'),\n",
       " ('что', 'SCONJ'),\n",
       " ('болеть', 'VERB'),\n",
       " ('очень', 'ADV'),\n",
       " ('отчетливо', 'ADV'),\n",
       " ('припомнить', 'VERB'),\n",
       " ('страшный', 'ADJ'),\n",
       " ('смерть', 'NOUN'),\n",
       " ('вызвать', 'VERB'),\n",
       " ('уже', 'ADV'),\n",
       " ('прежний', 'ADJ'),\n",
       " ('потрясение', 'NOUN'),\n",
       " ('оглядеться', 'VERB'),\n",
       " ('увидеть', 'VERB'),\n",
       " ('столик', 'NOUN'),\n",
       " ('кнопка', 'NOUN'),\n",
       " ('вовсе', 'ADV'),\n",
       " ('потому', 'ADV'),\n",
       " ('что', 'SCONJ'),\n",
       " ('нуждаться', 'VERB'),\n",
       " ('свой', 'DET'),\n",
       " ('привычка', 'NOUN'),\n",
       " ('надобность', 'NOUN'),\n",
       " ('трогать', 'VERB'),\n",
       " ('предмет', 'NOUN'),\n",
       " ('позвонить', 'VERB'),\n",
       " ('тотчас', 'ADV'),\n",
       " ('предстать', 'VERB'),\n",
       " ('толстый', 'ADJ'),\n",
       " ('женщина', 'NOUN'),\n",
       " ('белый', 'ADJ'),\n",
       " ('халат', 'NOUN'),\n",
       " ('нажать', 'VERB'),\n",
       " ('кнопка', 'NOUN'),\n",
       " ('стена', 'NOUN'),\n",
       " ('штора', 'NOUN'),\n",
       " ('уйти', 'VERB'),\n",
       " ('вверх', 'ADV'),\n",
       " ('комната', 'NOUN'),\n",
       " ('сразу', 'ADV'),\n",
       " ('посветлела', 'VERB'),\n",
       " ('лёгкий', 'ADJ'),\n",
       " ('решётка', 'NOUN'),\n",
       " ('отгораживать', 'VERB'),\n",
       " ('окно', 'NOUN'),\n",
       " ('увидеть', 'VERB'),\n",
       " ('чахлый', 'ADJ'),\n",
       " ('подмосковный', 'ADJ'),\n",
       " ('бор', 'NOUN'),\n",
       " ('понять', 'VERB'),\n",
       " ('что', 'SCONJ'),\n",
       " ('находиться', 'VERB'),\n",
       " ('город', 'NOUN'),\n",
       " ('пожалуйте', 'VERB'),\n",
       " ('ванна', 'NOUN'),\n",
       " ('брать', 'VERB'),\n",
       " ('пригласить', 'VERB'),\n",
       " ('женщина', 'NOUN'),\n",
       " ('словно', 'SCONJ'),\n",
       " ('волшебство', 'NOUN'),\n",
       " ('стена', 'NOUN'),\n",
       " ('уйти', 'VERB'),\n",
       " ('сторона', 'NOUN'),\n",
       " ('блеснули', 'VERB'),\n",
       " ('кран', 'NOUN'),\n",
       " ('взреветь', 'VERB'),\n",
       " ('где', 'ADV'),\n",
       " ('то', 'ADV'),\n",
       " ('вода', 'NOUN'),\n",
       " ('минута', 'NOUN'),\n",
       " ('быть', 'AUX'),\n",
       " ('гол', 'NOUN'),\n",
       " ('так', 'ADV'),\n",
       " ('как', 'SCONJ'),\n",
       " ('придерживаться', 'VERB'),\n",
       " ('мысль', 'NOUN'),\n",
       " ('что', 'SCONJ'),\n",
       " ('мужчина', 'NOUN'),\n",
       " ('стыдно', 'ADV'),\n",
       " ('купаться', 'VERB'),\n",
       " ('женщина', 'NOUN'),\n",
       " ('то', 'SCONJ'),\n",
       " ('ёжиться', 'VERB'),\n",
       " ('закрываться', 'VERB'),\n",
       " ('рука', 'NOUN'),\n",
       " ('женщина', 'NOUN'),\n",
       " ('заметить', 'VERB'),\n",
       " ('сделать', 'VERB'),\n",
       " ('вид', 'NOUN'),\n",
       " ('что', 'SCONJ'),\n",
       " ('смотреть', 'VERB'),\n",
       " ('поэт', 'NOUN'),\n",
       " ('тёплый', 'ADJ'),\n",
       " ('вода', 'NOUN'),\n",
       " ('понравиться', 'VERB'),\n",
       " ('поэт', 'NOUN'),\n",
       " ('вообще', 'ADV'),\n",
       " ('прежний', 'ADJ'),\n",
       " ('свой', 'DET'),\n",
       " ('жизнь', 'NOUN'),\n",
       " ('мыться', 'VERB'),\n",
       " ('почти', 'ADV'),\n",
       " ('никогда', 'ADV'),\n",
       " ('удержаться', 'VERB'),\n",
       " ('чтобы', 'SCONJ'),\n",
       " ('заметить', 'VERB'),\n",
       " ('ирония', 'NOUN'),\n",
       " ('как', 'SCONJ'),\n",
       " ('толстый', 'ADJ'),\n",
       " ('женщина', 'NOUN'),\n",
       " ('горделиво', 'ADV'),\n",
       " ('ответить', 'VERB'),\n",
       " ('нет', 'VERB'),\n",
       " ('гораздо', 'ADV'),\n",
       " ('хороший', 'ADJ'),\n",
       " ('граница', 'NOUN'),\n",
       " ('нет', 'VERB'),\n",
       " ('такой', 'DET'),\n",
       " ('лечебница', 'NOUN'),\n",
       " ('интурист', 'NOUN'),\n",
       " ('каждый', 'DET'),\n",
       " ('день', 'NOUN'),\n",
       " ('приезжать', 'VERB'),\n",
       " ('осматривать', 'VERB'),\n",
       " ('глянуть', 'VERB'),\n",
       " ('исподлобья', 'NOUN'),\n",
       " ('ответить', 'VERB'),\n",
       " ('весь', 'DET'),\n",
       " ('интурист', 'NOUN'),\n",
       " ('любить', 'VERB'),\n",
       " ('разный', 'ADJ'),\n",
       " ('попадаться', 'VERB'),\n",
       " ('Действительно', 'ADV'),\n",
       " ('было', 'AUX'),\n",
       " ('хороший', 'ADJ'),\n",
       " ('чем', 'SCONJ'),\n",
       " ('когда', 'SCONJ'),\n",
       " ('завтрак', 'NOUN'),\n",
       " ('вести', 'VERB'),\n",
       " ('коридор', 'NOUN'),\n",
       " ('осмотр', 'NOUN'),\n",
       " ('бедный', 'ADJ'),\n",
       " ('поэт', 'NOUN'),\n",
       " ('убедиться', 'VERB'),\n",
       " ('чист', 'NOUN'),\n",
       " ('беззвучный', 'ADJ'),\n",
       " ('этот', 'DET'),\n",
       " ('коридор', 'NOUN'),\n",
       " ('встреча', 'NOUN'),\n",
       " ('произойти', 'VERB'),\n",
       " ('случайно', 'ADV'),\n",
       " ('белый', 'ADJ'),\n",
       " ('дверь', 'NOUN'),\n",
       " ('вывести', 'VERB'),\n",
       " ('маленький', 'ADJ'),\n",
       " ('женщина', 'NOUN'),\n",
       " ('белый', 'ADJ'),\n",
       " ('халатик', 'NOUN'),\n",
       " ('увидеть', 'VERB'),\n",
       " ('взволноваться', 'VERB'),\n",
       " ('вынуть', 'VERB')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_and_tags = [token+\"_\"+tag for token, tag in raw_tokens_and_tags]\n",
    "print(len(tokens_and_tags))\n",
    "# tokens_and_tags\n",
    "raw_tokens_and_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'стравинского_NOUN' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_58405/519945903.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_and_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{raw_tokens_and_tags[i]}: {preproc.word2vec_rus.most_similar(tokens_and_tags[i], topn=3)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'стравинского_NOUN' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "for i in range(len(tokens_and_tags)):\n",
    "    print(f\"{raw_tokens_and_tags[i]}: {preproc.word2vec_rus.most_similar(tokens_and_tags[i], topn=3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every\n",
    "## Single\n",
    "# Word\n",
    "\n",
    "# Processed manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_rus = gensim.downloader.load('word2vec-ruscorpora-300')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def negative_coloration_remover(posit):\n",
    "    posit_full = posit[0]+\"_\"+posit[1]\n",
    "    return word2vec_rus.most_similar([posit_full, \"хороший_ADJ\"], negative=[\"плохой_ADJ\"])[0][0].split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [] \n",
    "for i in raw_tokens_and_tags:\n",
    "    try:\n",
    "        res.append([i, negative_coloration_remover(i)])\n",
    "    except:\n",
    "        pass\n",
    "res = res[:100]\n",
    "len(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An attempt in removing negative coloration from words \n",
    "## (not all words were found in the model, so some of them were removed on preproc stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"un-negatified words\", \"w\") as file:\n",
    "    for i in res:\n",
    "        file.write(f\"\\\"{i[0][0]}\\\" was automatically un-negatified into \\\"{i[1]}\\\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
