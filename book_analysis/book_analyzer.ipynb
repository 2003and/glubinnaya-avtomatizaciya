{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo list:\n",
    "- ~~book reading function~~\n",
    "- morphologic analysis:\n",
    "    - ~~lexical diversity calculation function~~\n",
    "    - ~~parts of speech counting function (by word's initial form)~~\n",
    "    - ~~parts of speech counting function (by tags, part of speech types)~~\n",
    "    - ~~text dynamics calculation functiion~~\n",
    "    - *top-1 for every speech part\n",
    "- syntax analysis\n",
    "    - reduce the amount of text\n",
    "    - ~~most popular root word~~\n",
    "- semantic analysis\n",
    "    - ~~stop-word cleanup~~\n",
    "    - *automatic tag generation\n",
    "- graphic demonstration\n",
    "    - tag cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/andrew/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pip install -q nltk pymorphy2 gensim\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "import spacy\n",
    "import numpy as np\n",
    "import gensim.downloader\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load('ru_core_news_sm')\n",
    "# word2vec_rus = gensim.downloader.load('word2vec-ruscorpora-300')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a few constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT = ('.',',',':',';','\\'','\"','-','(',')','!','?','...','$','№')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preproc:\n",
    "    def __init__(self, book_name):\n",
    "        self.text = self.get_book_text(book_name)\n",
    "        self.tokens = nltk.word_tokenize(self.text)\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        self.tokens_no_punct = nltk.tokenize.RegexpTokenizer(r\"\\w+\").tokenize(self.text)\n",
    "        self.nlp = spacy.load('ru_core_news_sm')\n",
    "        self.word2vec_rus = gensim.downloader.load('word2vec-ruscorpora-300')\n",
    "        self.doc = nlp(self.text)\n",
    "        self.lemm = nltk.WordNetLemmatizer()\n",
    "        self.lex_diversity()\n",
    "        self.make_tags()\n",
    "    \n",
    "    def process_stopwords(self, overwrite=False):\n",
    "        stopw = stopwords.words(\"russian\")\n",
    "        if overwrite:\n",
    "            self.tokens = [token for token in self.tokens if token not in stopw]\n",
    "        else:\n",
    "            self.tokens_no_stopwords = [token for token in self.tokens if token not in stopw]\n",
    "\n",
    "    def get_book_text(self, name):\n",
    "        with open(name, \"r\") as raw:\n",
    "            text = \"\"\n",
    "            for t in raw.readlines():\n",
    "                text += t+\"\\n\"\n",
    "            return text\n",
    "    \n",
    "    def lex_diversity(self):\n",
    "        unique_words_set = set(self.tokens)\n",
    "        self.lex_diversity_coeff = len(unique_words_set) / len(self.tokens)\n",
    "\n",
    "    def make_tags(self):\n",
    "        self.tagged = []\n",
    "        self.tokens_and_tags = []\n",
    "        for token in self.tokens_no_punct:\n",
    "            tag = self.morph.parse(token)[0].tag.POS\n",
    "            self.tagged.append((token, tag))\n",
    "            # self.tokens_and_tags.append(self.lemm.lemmatize(token).lower()+'_'+tag)\n",
    "            # spacy find all собственные имена\n",
    "        return self.tagged\n",
    "\n",
    "    def text_dynamics(total_tags: list):\n",
    "        verbs = total_tags.count(\"VERB\")\n",
    "        return verbs / len(total_tags)\n",
    "\n",
    "    def text_dynamics_dict(self):\n",
    "        self.count_speech_parts()\n",
    "        verbs = self.tags_count[\"VERB\"]\n",
    "        total = 0\n",
    "        for i in self.tags_count.keys():\n",
    "            total += self.tags_count[i]\n",
    "        return verbs/total\n",
    "    \n",
    "    def count_speech_parts(self):\n",
    "        self.make_tags()\n",
    "        res = dict()\n",
    "        for token in self.tagged:\n",
    "            if (token[1] in res.keys()) :\n",
    "                res[token[1]] += 1\n",
    "            else:\n",
    "                res[token[1]] = 1\n",
    "        self.tags_count = res\n",
    "\n",
    "    def count_unique_words(self):\n",
    "        self.make_tags()\n",
    "        res = dict()\n",
    "        for token in self.tagged:\n",
    "            if not (token[1] in (\"CONJ\", \"PREP\", \"PRCL\")):\n",
    "                if token[0] in res.keys():\n",
    "                    res[token[0]] += 1\n",
    "                else:\n",
    "                    res[token[0]] = 1\n",
    "        self.word_count = res\n",
    "        self.word_count_sorted = dict(sorted(self.word_count.items(), key=lambda item: item[1]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_books(self, book1, book2):\n",
    "        text1 = self.get_book_text(book1)\n",
    "        text2 = self.get_book_text(book2)\n",
    "\n",
    "        doc1 = self.nlp(text1)\n",
    "        doc2 = self.nlp(text2)\n",
    "        return (doc1.similarity(doc2), np.dot(doc1.vector, doc2.vector) / (np.linalg.norm(doc1.vector) * (np.linalg.norm(doc2.vector))))\n",
    "\n",
    "    def count_root_speech_parts(self):\n",
    "        res = dict()\n",
    "        for token in self.doc:\n",
    "            if token.pos_ not in (\"SPACE\", \"PUNCT\"):\n",
    "                if token.pos_ not in res.keys():\n",
    "                    res[token.pos_] = 1\n",
    "                else:\n",
    "                    res[token.pos_] += 1\n",
    "        self.root_count = res\n",
    "        self.root_count_sorted = dict(sorted(self.root_count.items(), key=lambda item: item[1]))\n",
    "    \n",
    "    def fetch_tokens(self, amount=100, index=0):\n",
    "        return [self.tokens[token] for token in range(index, amount)]\n",
    "\n",
    "    def get_tokens_and_tags(self, amount=100):\n",
    "        res = []\n",
    "        i = 0\n",
    "        while len(res) < amount and i < len(self.doc):\n",
    "        # for i in range(min(amount, len(self.doc))):\n",
    "            if self.doc[i].tag_ not in (\"PUNCT\", \"SPACE\", \"PROPN\", \"ADP\", \"NUM\", \"CCONJ\", \"PART\", \"PRON\") and self.doc[i].lemma_ != \"-\":\n",
    "                res.append((self.doc[i].lemma_, self.doc[i].tag_))\n",
    "            i += 1\n",
    "        \n",
    "        return res\n",
    "\n",
    "    def check_vectors(self, first_n_elements=100, logging=False):\n",
    "        res = []\n",
    "        i = 0\n",
    "        while len(res) < first_n_elements:\n",
    "            try:\n",
    "                res.append(self.word2vec_rus[self.doc[i].lemma_+'_'+self.tagged[i][1]]) # word2vec_rus[\"тест_NOUN\"]\n",
    "            except:\n",
    "                if logging:\n",
    "                    print(f\"error parsing {self.doc[i].lemma_+'_'+self.tagged[i][1]}\")\n",
    "                # res.append(self.doc[i].lemma_)\n",
    "            i += 1\n",
    "        return res\n",
    "    \n",
    "    def metamorph(self, first_n_elements=100):\n",
    "        res = \"\"\n",
    "        for i in self.check_vectors(first_n_elements):\n",
    "            res += self.word2vec_rus.most_similar(positive=[\"женщина_NOUN\", i], negative=\"мужчина_NOUN\")[-1][0].split(\"_\")[0] + \" \"\n",
    "        return res\n",
    "    \n",
    "    # def antonimize(word_list):\n",
    "    #     antonyms = []\n",
    "    #     for word in word_list:\n",
    "    #         synsets = wordnet.synsets(word)\n",
    "    #         for synset in synsets:\n",
    "    #             for lemma in synset.lemmas():\n",
    "    #                 if lemma.antonyms():\n",
    "    #                     antonyms.append(lemma.antonyms()[0].name())\n",
    "    #                     break\n",
    "    #     return antonyms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now onto the fun part\n",
    "# Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_58405/3509443336.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlist_of_books\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Похождения Чичикова (1921)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Белая гвардия (1922)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Роковые яйца (1924)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Собачье сердце (1925)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Мастер и маргарита (1929)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Театральный роман (1936)\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_of_books\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpreproc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPreproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_out\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mpreproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_58405/2001562569.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, book_name)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmorph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpymorphy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMorphAnalyzer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_no_punct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegexpTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"\\w+\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ru_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2vec_rus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word2vec-ruscorpora-300'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \"\"\"\n\u001b[0;32m---> 51\u001b[0;31m     return util.load_model(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mget_lang_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"blank:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \"\"\"\n\u001b[1;32m    500\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/ru_core_news_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m     return load_model_from_path(\n\u001b[0m\u001b[1;32m    683\u001b[0m         \u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0mvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m     )\n\u001b[0;32m--> 547\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, exclude, overrides)\u001b[0m\n\u001b[1;32m   2182\u001b[0m             \u001b[0;31m# Convert to list here in case exclude is (default) tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m  \u001b[0;31m# type: ignore[assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_link_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0;31m# Split to support file names like meta.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m             \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mdeserialize_vocab\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdeserialize_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.from_disk\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/vectors.pyx\u001b[0m in \u001b[0;36mspacy.vectors.Vectors.from_disk\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m def from_disk(\n\u001b[0m\u001b[1;32m   1364\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m     \u001b[0mreaders\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_of_books = [\"Похождения Чичикова (1921)\",\"Белая гвардия (1922)\",\"Роковые яйца (1924)\",\"Собачье сердце (1925)\",\"Мастер и маргарита (1929)\",\"Театральный роман (1936)\"]\n",
    "for name in list_of_books:\n",
    "    preproc = Preproc(name)\n",
    "    with open(name+\"_out\", \"w\") as file:\n",
    "        preproc.process_stopwords()\n",
    "        preproc.count_unique_words()\n",
    "        preproc.count_speech_parts()\n",
    "        preproc.count_root_speech_parts()\n",
    "        # if name == list_of_books[0]:\n",
    "        print(f\"Metamorphed \\\"{name}\\\":\\n\",preproc.metamorph(), \"\\n\")\n",
    "        \n",
    "        file.write(f\"Lexical diversity: {str(preproc.lex_diversity_coeff)}\\n\")\n",
    "\n",
    "        file.write(\"Top-3 words: \\n\")\n",
    "        file.write(f\"\\t{list(preproc.word_count_sorted.keys())[-1]}\\n\")\n",
    "        file.write(f\"\\t{list(preproc.word_count_sorted.keys())[-2]}\\n\")\n",
    "        file.write(f\"\\t{list(preproc.word_count_sorted.keys())[-3]}\\n\")\n",
    "\n",
    "        file.write(f\"Text dynamics: {preproc.text_dynamics_dict()}\\n\")\n",
    "\n",
    "        file.write(f\"Top root speech part: {list(preproc.root_count_sorted.keys())[-1]}\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual word2vec words processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc = Preproc(list_of_books[4])\n",
    "preproc.process_stopwords(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_tokens_and_tags = preproc.get_tokens_and_tags(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('стравинского', 'NOUN'),\n",
       " ('тот', 'DET'),\n",
       " ('время', 'NOUN'),\n",
       " ('как', 'SCONJ'),\n",
       " ('раз', 'NOUN'),\n",
       " ('как', 'ADV'),\n",
       " ('вести', 'VERB'),\n",
       " ('бездомный', 'ADJ'),\n",
       " ('долгий', 'ADJ'),\n",
       " ('сон', 'NOUN'),\n",
       " ('открыть', 'VERB'),\n",
       " ('глаз', 'NOUN'),\n",
       " ('некоторый', 'DET'),\n",
       " ('время', 'NOUN'),\n",
       " ('соображать', 'VERB'),\n",
       " ('как', 'SCONJ'),\n",
       " ('попасть', 'VERB'),\n",
       " ('этот', 'DET'),\n",
       " ('необыкновенный', 'ADJ'),\n",
       " ('комната', 'NOUN'),\n",
       " ('чистый', 'ADJ'),\n",
       " ('белый', 'ADJ'),\n",
       " ('стена', 'NOUN'),\n",
       " ('удивительный', 'ADJ'),\n",
       " ('ночной', 'ADJ'),\n",
       " ('столик', 'NOUN'),\n",
       " ('сделать', 'VERB'),\n",
       " ('какой', 'DET'),\n",
       " ('то', 'DET'),\n",
       " ('неизвестный', 'ADJ'),\n",
       " ('светлый', 'ADJ'),\n",
       " ('металл', 'NOUN'),\n",
       " ('величественной', 'VERB'),\n",
       " ('белый', 'ADJ'),\n",
       " ('штора', 'NOUN'),\n",
       " ('весь', 'DET'),\n",
       " ('стена', 'NOUN'),\n",
       " ('тряхнуть', 'VERB'),\n",
       " ('голова', 'NOUN'),\n",
       " ('убедиться', 'VERB'),\n",
       " ('что', 'SCONJ'),\n",
       " ('болеть', 'VERB'),\n",
       " ('очень', 'ADV'),\n",
       " ('отчетливо', 'ADV'),\n",
       " ('припомнить', 'VERB'),\n",
       " ('страшный', 'ADJ'),\n",
       " ('смерть', 'NOUN'),\n",
       " ('вызвать', 'VERB'),\n",
       " ('уже', 'ADV'),\n",
       " ('прежний', 'ADJ'),\n",
       " ('потрясение', 'NOUN'),\n",
       " ('оглядеться', 'VERB'),\n",
       " ('увидеть', 'VERB'),\n",
       " ('столик', 'NOUN'),\n",
       " ('кнопка', 'NOUN'),\n",
       " ('вовсе', 'ADV'),\n",
       " ('потому', 'ADV'),\n",
       " ('что', 'SCONJ'),\n",
       " ('нуждаться', 'VERB'),\n",
       " ('свой', 'DET'),\n",
       " ('привычка', 'NOUN'),\n",
       " ('надобность', 'NOUN'),\n",
       " ('трогать', 'VERB'),\n",
       " ('предмет', 'NOUN'),\n",
       " ('позвонить', 'VERB'),\n",
       " ('тотчас', 'ADV'),\n",
       " ('предстать', 'VERB'),\n",
       " ('толстый', 'ADJ'),\n",
       " ('женщина', 'NOUN'),\n",
       " ('белый', 'ADJ'),\n",
       " ('халат', 'NOUN'),\n",
       " ('нажать', 'VERB'),\n",
       " ('кнопка', 'NOUN'),\n",
       " ('стена', 'NOUN'),\n",
       " ('штора', 'NOUN'),\n",
       " ('уйти', 'VERB'),\n",
       " ('вверх', 'ADV'),\n",
       " ('комната', 'NOUN'),\n",
       " ('сразу', 'ADV'),\n",
       " ('посветлела', 'VERB'),\n",
       " ('лёгкий', 'ADJ'),\n",
       " ('решётка', 'NOUN'),\n",
       " ('отгораживать', 'VERB'),\n",
       " ('окно', 'NOUN'),\n",
       " ('увидеть', 'VERB'),\n",
       " ('чахлый', 'ADJ'),\n",
       " ('подмосковный', 'ADJ'),\n",
       " ('бор', 'NOUN'),\n",
       " ('понять', 'VERB'),\n",
       " ('что', 'SCONJ'),\n",
       " ('находиться', 'VERB'),\n",
       " ('город', 'NOUN'),\n",
       " ('пожалуйте', 'VERB'),\n",
       " ('ванна', 'NOUN'),\n",
       " ('брать', 'VERB'),\n",
       " ('пригласить', 'VERB'),\n",
       " ('женщина', 'NOUN'),\n",
       " ('словно', 'SCONJ'),\n",
       " ('волшебство', 'NOUN'),\n",
       " ('стена', 'NOUN'),\n",
       " ('уйти', 'VERB'),\n",
       " ('сторона', 'NOUN'),\n",
       " ('блеснули', 'VERB'),\n",
       " ('кран', 'NOUN'),\n",
       " ('взреветь', 'VERB'),\n",
       " ('где', 'ADV'),\n",
       " ('то', 'ADV'),\n",
       " ('вода', 'NOUN'),\n",
       " ('минута', 'NOUN'),\n",
       " ('быть', 'AUX'),\n",
       " ('гол', 'NOUN'),\n",
       " ('так', 'ADV'),\n",
       " ('как', 'SCONJ'),\n",
       " ('придерживаться', 'VERB'),\n",
       " ('мысль', 'NOUN'),\n",
       " ('что', 'SCONJ'),\n",
       " ('мужчина', 'NOUN'),\n",
       " ('стыдно', 'ADV'),\n",
       " ('купаться', 'VERB'),\n",
       " ('женщина', 'NOUN'),\n",
       " ('то', 'SCONJ'),\n",
       " ('ёжиться', 'VERB'),\n",
       " ('закрываться', 'VERB'),\n",
       " ('рука', 'NOUN'),\n",
       " ('женщина', 'NOUN'),\n",
       " ('заметить', 'VERB'),\n",
       " ('сделать', 'VERB'),\n",
       " ('вид', 'NOUN'),\n",
       " ('что', 'SCONJ'),\n",
       " ('смотреть', 'VERB'),\n",
       " ('поэт', 'NOUN'),\n",
       " ('тёплый', 'ADJ'),\n",
       " ('вода', 'NOUN'),\n",
       " ('понравиться', 'VERB'),\n",
       " ('поэт', 'NOUN'),\n",
       " ('вообще', 'ADV'),\n",
       " ('прежний', 'ADJ'),\n",
       " ('свой', 'DET'),\n",
       " ('жизнь', 'NOUN'),\n",
       " ('мыться', 'VERB'),\n",
       " ('почти', 'ADV'),\n",
       " ('никогда', 'ADV'),\n",
       " ('удержаться', 'VERB'),\n",
       " ('чтобы', 'SCONJ'),\n",
       " ('заметить', 'VERB'),\n",
       " ('ирония', 'NOUN'),\n",
       " ('как', 'SCONJ'),\n",
       " ('толстый', 'ADJ'),\n",
       " ('женщина', 'NOUN'),\n",
       " ('горделиво', 'ADV'),\n",
       " ('ответить', 'VERB'),\n",
       " ('нет', 'VERB'),\n",
       " ('гораздо', 'ADV'),\n",
       " ('хороший', 'ADJ'),\n",
       " ('граница', 'NOUN'),\n",
       " ('нет', 'VERB'),\n",
       " ('такой', 'DET'),\n",
       " ('лечебница', 'NOUN'),\n",
       " ('интурист', 'NOUN'),\n",
       " ('каждый', 'DET'),\n",
       " ('день', 'NOUN'),\n",
       " ('приезжать', 'VERB'),\n",
       " ('осматривать', 'VERB'),\n",
       " ('глянуть', 'VERB'),\n",
       " ('исподлобья', 'NOUN'),\n",
       " ('ответить', 'VERB'),\n",
       " ('весь', 'DET'),\n",
       " ('интурист', 'NOUN'),\n",
       " ('любить', 'VERB'),\n",
       " ('разный', 'ADJ'),\n",
       " ('попадаться', 'VERB'),\n",
       " ('Действительно', 'ADV'),\n",
       " ('было', 'AUX'),\n",
       " ('хороший', 'ADJ'),\n",
       " ('чем', 'SCONJ'),\n",
       " ('когда', 'SCONJ'),\n",
       " ('завтрак', 'NOUN'),\n",
       " ('вести', 'VERB'),\n",
       " ('коридор', 'NOUN'),\n",
       " ('осмотр', 'NOUN'),\n",
       " ('бедный', 'ADJ'),\n",
       " ('поэт', 'NOUN'),\n",
       " ('убедиться', 'VERB'),\n",
       " ('чист', 'NOUN'),\n",
       " ('беззвучный', 'ADJ'),\n",
       " ('этот', 'DET'),\n",
       " ('коридор', 'NOUN'),\n",
       " ('встреча', 'NOUN'),\n",
       " ('произойти', 'VERB'),\n",
       " ('случайно', 'ADV'),\n",
       " ('белый', 'ADJ'),\n",
       " ('дверь', 'NOUN'),\n",
       " ('вывести', 'VERB'),\n",
       " ('маленький', 'ADJ'),\n",
       " ('женщина', 'NOUN'),\n",
       " ('белый', 'ADJ'),\n",
       " ('халатик', 'NOUN'),\n",
       " ('увидеть', 'VERB'),\n",
       " ('взволноваться', 'VERB'),\n",
       " ('вынуть', 'VERB')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_and_tags = [token+\"_\"+tag for token, tag in raw_tokens_and_tags]\n",
    "print(len(tokens_and_tags))\n",
    "# tokens_and_tags\n",
    "raw_tokens_and_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Key 'стравинского_NOUN' not present in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_58405/519945903.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_and_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{raw_tokens_and_tags[i]}: {preproc.word2vec_rus.most_similar(tokens_and_tags[i], topn=3)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    839\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0;31m# compute the weighted average of all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_mean_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_normalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         all_keys = [\n\u001b[1;32m    843\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_KEY_TYPES\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    516\u001b[0m                 \u001b[0mtotal_weight\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Key '{key}' not present in vocabulary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtotal_weight\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Key 'стравинского_NOUN' not present in vocabulary\""
     ]
    }
   ],
   "source": [
    "for i in range(len(tokens_and_tags)):\n",
    "    print(f\"{raw_tokens_and_tags[i]}: {preproc.word2vec_rus.most_similar(tokens_and_tags[i], topn=3)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Every\n",
    "## Single\n",
    "# Word\n",
    "\n",
    "# Processed manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_rus = gensim.downloader.load('word2vec-ruscorpora-300')\n",
    "def temp_util(posit, negat):\n",
    "    posit_full = posit[0]+\"_\"+posit[1]\n",
    "    negat_full = negat[0]+\"_\"+negat[1]\n",
    "    return word2vec_rus.most_similar([posit_full], negative=[negat_full])[0][0].split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01819149,  0.0562468 ,  0.07606095, -0.00676807, -0.10294734,\n",
       "        0.04881339,  0.04232221, -0.04779554,  0.03307107, -0.09510215,\n",
       "       -0.01953268, -0.08449668,  0.00179145, -0.04572642,  0.00145962,\n",
       "       -0.01562569,  0.01368144,  0.09611773,  0.05821797, -0.02356944,\n",
       "       -0.00277603, -0.01740462, -0.0233964 ,  0.00372298,  0.02813967,\n",
       "        0.01722571, -0.00393285, -0.01037687, -0.05924214, -0.02488719,\n",
       "        0.0061308 , -0.0384831 ,  0.06257668, -0.04703574,  0.04653278,\n",
       "        0.07771305, -0.06432268,  0.05496893, -0.01091879,  0.06191562,\n",
       "        0.12634777,  0.13306184, -0.04712044, -0.05431946,  0.00274291,\n",
       "        0.05026461,  0.16297658, -0.02625923, -0.05225719, -0.03181458,\n",
       "        0.0740444 ,  0.04330749, -0.00929615, -0.03485155,  0.05152824,\n",
       "       -0.02629252, -0.08197249, -0.15417993,  0.01334044, -0.00686578,\n",
       "        0.03134248,  0.00793455,  0.13912599, -0.02372578,  0.03415758,\n",
       "        0.00103617, -0.0987834 , -0.14136644,  0.0350162 ,  0.02153468,\n",
       "       -0.0753947 ,  0.02166004, -0.00398254, -0.02958264, -0.06615303,\n",
       "        0.03409165,  0.00222688,  0.01734903,  0.02733443,  0.07212944,\n",
       "       -0.06769796, -0.01285805, -0.10291747, -0.02136617,  0.0331155 ,\n",
       "        0.03909008, -0.01957544, -0.04174962,  0.04298959,  0.00667252,\n",
       "        0.01316659, -0.03555491, -0.02056522, -0.06037698, -0.01269127,\n",
       "        0.11422285, -0.04378754, -0.00406041, -0.00756479, -0.00746394,\n",
       "        0.01436055, -0.05679331, -0.10998904, -0.02652654,  0.00477864,\n",
       "       -0.0240123 , -0.15371682, -0.07338709, -0.02298927,  0.04184983,\n",
       "        0.01006666, -0.03803285, -0.0601971 ,  0.02392506, -0.02709032,\n",
       "        0.0334755 , -0.0285465 ,  0.05684511,  0.09365261,  0.03194503,\n",
       "        0.00026985, -0.07127941,  0.02471258, -0.05158358,  0.01200574,\n",
       "       -0.01158906, -0.05280666, -0.02664425,  0.00718268,  0.04694177,\n",
       "       -0.0333183 , -0.01874598, -0.03810688,  0.07099648, -0.07032768,\n",
       "        0.03491798, -0.07690188, -0.04997808, -0.0160573 , -0.09831399,\n",
       "        0.0038714 ,  0.01902993, -0.114683  ,  0.00814698, -0.0224475 ,\n",
       "        0.08916172, -0.12510453, -0.04628708, -0.00972442,  0.05555638,\n",
       "       -0.08316404,  0.05552014, -0.02559969, -0.01629643, -0.10484055,\n",
       "        0.0258381 , -0.00477789, -0.0463768 ,  0.00731701, -0.00064419,\n",
       "        0.07059913, -0.01044969, -0.03279572, -0.0525666 , -0.03136668,\n",
       "       -0.08437734, -0.03406637, -0.08330045, -0.06305958, -0.03098783,\n",
       "       -0.02258648, -0.09293788, -0.00616826, -0.03086825, -0.02964749,\n",
       "       -0.03602858,  0.13078386,  0.10397039, -0.08283884, -0.00180483,\n",
       "       -0.084202  ,  0.01752164,  0.01421689,  0.06058497, -0.02498042,\n",
       "       -0.02879446,  0.08711655, -0.00241371, -0.09198503,  0.02623015,\n",
       "       -0.02695033,  0.04039916, -0.0795232 , -0.03005467, -0.01277635,\n",
       "        0.06561197, -0.04892513, -0.00987036,  0.04095499, -0.02148877,\n",
       "        0.05977089,  0.00022573, -0.02632475, -0.06298403, -0.10786328,\n",
       "       -0.07493272, -0.18522273,  0.00771914, -0.04843938, -0.00833369,\n",
       "       -0.07310506,  0.03383885,  0.01187966,  0.0026053 ,  0.05250644,\n",
       "        0.06349935, -0.02239955,  0.00681   , -0.01503293, -0.02466302,\n",
       "       -0.08155487,  0.0587787 ,  0.04555636, -0.07639334,  0.00336137,\n",
       "       -0.03762328, -0.05377711,  0.04818192, -0.04676313, -0.05202001,\n",
       "        0.06545825,  0.03131141, -0.07981748,  0.06527869, -0.08469184,\n",
       "        0.02160561, -0.05175259,  0.02407119, -0.13581948, -0.01086401,\n",
       "        0.02917871,  0.05011566, -0.00633142, -0.00924979, -0.03535365,\n",
       "        0.05505849,  0.0591964 , -0.0565738 ,  0.01586373, -0.03705903,\n",
       "       -0.04761956, -0.09588622,  0.01348917, -0.03954075, -0.00194128,\n",
       "       -0.11874016,  0.0785996 ,  0.08739931,  0.05762653, -0.038925  ,\n",
       "       -0.05432635, -0.02473527, -0.02743051, -0.11244998, -0.07644275,\n",
       "        0.07978257,  0.01302182, -0.02067713, -0.08423469,  0.00141233,\n",
       "        0.05266   , -0.01201177, -0.04315435,  0.17355359, -0.03276477,\n",
       "        0.04839146,  0.0230032 , -0.00618295,  0.04299096,  0.00019427,\n",
       "       -0.06154682,  0.05156872,  0.01641317,  0.00905168, -0.07582542,\n",
       "       -0.03103687, -0.04289345,  0.02015849,  0.04428823,  0.05094643,\n",
       "        0.04755275, -0.00676639,  0.00150193, -0.08320487, -0.07154516,\n",
       "       -0.09855817,  0.00680691,  0.15752032, -0.03346088, -0.06947856],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = [] \n",
    "for i in raw_tokens_and_tags:\n",
    "    try:\n",
    "        res.append([i, temp_util(i, [\"плохой\", \"ADJ\"])])\n",
    "    except:\n",
    "        pass\n",
    "res = res[:100]\n",
    "len(res)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An attempt in removing negative coloration from words \n",
    "## (not all words were found in the model, so some of them were removed on preproc stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"время\" was transformed into \"промежуток\"\n",
      "\"вести\" was transformed into \"вестись\"\n",
      "\"бездомный\" was transformed into \"бездомный\"\n",
      "\"долгий\" was transformed into \"недолгий\"\n",
      "\"сон\" was transformed into \"сновидение\"\n",
      "\"глаз\" was transformed into \"взор\"\n",
      "\"некоторый\" was transformed into \"некоторые\"\n",
      "\"время\" was transformed into \"промежуток\"\n",
      "\"соображать\" was transformed into \"прикидывать\"\n",
      "\"необыкновенный\" was transformed into \"необычайный\"\n",
      "\"комната\" was transformed into \"спальня\"\n",
      "\"чистый\" was transformed into \"девственный\"\n",
      "\"белый\" was transformed into \"белоснежный\"\n",
      "\"стена\" was transformed into \"простенок\"\n",
      "\"удивительный\" was transformed into \"необычайный\"\n",
      "\"ночной\" was transformed into \"дневной\"\n",
      "\"столик\" was transformed into \"стол\"\n",
      "\"сделать\" was transformed into \"делать\"\n",
      "\"неизвестный\" was transformed into \"неведомый\"\n",
      "\"светлый\" was transformed into \"сиять\"\n",
      "\"металл\" was transformed into \"медь\"\n",
      "\"белый\" was transformed into \"белоснежный\"\n",
      "\"штора\" was transformed into \"жалюзи\"\n",
      "\"весь\" was transformed into \"необъятный\"\n",
      "\"стена\" was transformed into \"простенок\"\n",
      "\"тряхнуть\" was transformed into \"встряхивать\"\n",
      "\"голова\" was transformed into \"темя\"\n",
      "\"болеть\" was transformed into \"заболеть\"\n",
      "\"очень\" was transformed into \"чрезвычайно\"\n",
      "\"отчетливо\" was transformed into \"четко\"\n",
      "\"страшный\" was transformed into \"оцепенять\"\n",
      "\"смерть\" was transformed into \"кончина\"\n",
      "\"прежний\" was transformed into \"былой\"\n",
      "\"потрясение\" was transformed into \"катаклизм\"\n",
      "\"оглядеться\" was transformed into \"осматриваться\"\n",
      "\"увидеть\" was transformed into \"увидать\"\n",
      "\"столик\" was transformed into \"стол\"\n",
      "\"кнопка\" was transformed into \"нажимать\"\n",
      "\"вовсе\" was transformed into \"совершенно\"\n",
      "\"нуждаться\" was transformed into \"требовать\"\n",
      "\"привычка\" was transformed into \"обычай\"\n",
      "\"надобность\" was transformed into \"нужда\"\n",
      "\"трогать\" was transformed into \"тронуть\"\n",
      "\"предмет\" was transformed into \"рассматривание\"\n",
      "\"позвонить\" was transformed into \"звонить\"\n",
      "\"тотчас\" was transformed into \"немедленно\"\n",
      "\"толстый\" was transformed into \"пухлый\"\n",
      "\"женщина\" was transformed into \"мужчина\"\n",
      "\"белый\" was transformed into \"белоснежный\"\n",
      "\"халат\" was transformed into \"халатик\"\n",
      "\"кнопка\" was transformed into \"нажимать\"\n",
      "\"стена\" was transformed into \"простенок\"\n",
      "\"штора\" was transformed into \"жалюзи\"\n",
      "\"вверх\" was transformed into \"вниз\"\n",
      "\"комната\" was transformed into \"спальня\"\n",
      "\"сразу\" was transformed into \"мгновенно\"\n",
      "\"отгораживать\" was transformed into \"огораживать\"\n",
      "\"окно\" was transformed into \"окошко\"\n",
      "\"увидеть\" was transformed into \"увидать\"\n",
      "\"чахлый\" was transformed into \"деревце\"\n",
      "\"подмосковный\" was transformed into \"подмосковье\"\n",
      "\"бор\" was transformed into \"гейзенберг\"\n",
      "\"находиться\" was transformed into \"располагать\"\n",
      "\"город\" was transformed into \"столица\"\n",
      "\"ванна\" was transformed into \"ванная\"\n",
      "\"брать\" was transformed into \"взять\"\n",
      "\"женщина\" was transformed into \"мужчина\"\n",
      "\"волшебство\" was transformed into \"магический\"\n",
      "\"стена\" was transformed into \"простенок\"\n",
      "\"сторона\" was transformed into \"направление\"\n",
      "\"кран\" was transformed into \"краник\"\n",
      "\"взреветь\" was transformed into \"зареветь\"\n",
      "\"вода\" was transformed into \"вод\"\n",
      "\"минута\" was transformed into \"секунда\"\n",
      "\"гол\" was transformed into \"хавтайм\"\n",
      "\"придерживаться\" was transformed into \"опираться\"\n",
      "\"мысль\" was transformed into \"идея\"\n",
      "\"мужчина\" was transformed into \"женщина\"\n",
      "\"стыдно\" was transformed into \"совестно\"\n",
      "\"купаться\" was transformed into \"купание\"\n",
      "\"женщина\" was transformed into \"мужчина\"\n",
      "\"закрываться\" was transformed into \"закрывать\"\n",
      "\"рука\" was transformed into \"ладонь\"\n",
      "\"женщина\" was transformed into \"мужчина\"\n",
      "\"сделать\" was transformed into \"делать\"\n",
      "\"вид\" was transformed into \"экдистероидсодержащий\"\n",
      "\"смотреть\" was transformed into \"глядеть\"\n",
      "\"поэт\" was transformed into \"поэзия\"\n",
      "\"вода\" was transformed into \"вод\"\n",
      "\"понравиться\" was transformed into \"нравиться\"\n",
      "\"поэт\" was transformed into \"поэзия\"\n",
      "\"вообще\" was transformed into \"собственно\"\n",
      "\"прежний\" was transformed into \"былой\"\n",
      "\"жизнь\" was transformed into \"жизненный\"\n",
      "\"мыться\" was transformed into \"умываться\"\n",
      "\"ирония\" was transformed into \"сарказм\"\n",
      "\"толстый\" was transformed into \"пухлый\"\n",
      "\"женщина\" was transformed into \"мужчина\"\n",
      "\"горделиво\" was transformed into \"гордо\"\n",
      "\"гораздо\" was transformed into \"несравненно\"\n"
     ]
    }
   ],
   "source": [
    "for i in res:\n",
    "    print(f\"\\\"{i[0][0]}\\\" was transformed into \\\"{i[1]}\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
