{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo list:\n",
    "- ~~book reading function~~\n",
    "- morphologic analysis:\n",
    "    - ~~lexical diversity calculation function~~\n",
    "    - ~~parts of speech counting function (by word's initial form)~~\n",
    "    - ~~parts of speech counting function (by tags, part of speech types)~~\n",
    "    - ~~text dynamics calculation functiion~~\n",
    "    - *top-1 for every speech part\n",
    "- syntax analysis\n",
    "    - reduce the amount of text\n",
    "    - ~~most popular root word~~\n",
    "- semantic analysis\n",
    "    - stop-word cleanup\n",
    "    - *automatic tag generation\n",
    "- graphic demonstration\n",
    "    - tag cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/andrew/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pip install -q nltk pymorphy2\n",
    "import nltk\n",
    "import pymorphy2\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load('ru_core_news_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a few constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT = ('.',',',':',';','\\'','\"','-','(',')','!','?','...','$','№')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preproc:\n",
    "    def __init__(self, book_name):\n",
    "        self.text = self.get_book_text(book_name)\n",
    "        self.tokens = nltk.word_tokenize(self.text)\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        self.tokens_no_punct = nltk.tokenize.RegexpTokenizer(r\"\\w+\").tokenize(self.text)\n",
    "        self.nlp = spacy.load('ru_core_news_sm')\n",
    "        self.doc = nlp(self.text)\n",
    "        self.lex_diversity()\n",
    "        self.make_tags()\n",
    "    \n",
    "    def process_stopwords(self, overwrite=False):\n",
    "        stopwords = stopwords.words(\"russian\")\n",
    "        if overwrite:\n",
    "            pass\n",
    "        else:\n",
    "            self.tokens_no_stopwords = [token for token in self.tokens if token not in stopwords]\n",
    "\n",
    "    def get_book_text(self, name):\n",
    "        with open(name, \"r\") as raw:\n",
    "            text = \"\"\n",
    "            for t in raw.readlines():\n",
    "                text += t+\"\\n\"\n",
    "            return text\n",
    "    \n",
    "    def lex_diversity(self):\n",
    "        unique_words_set = set(self.tokens)\n",
    "        self.lex_diversity_coeff = len(unique_words_set) / len(self.tokens)\n",
    "\n",
    "    def make_tags(self):\n",
    "        self.tagged = []\n",
    "        for token in self.tokens_no_punct:\n",
    "            self.tagged.append((token, self.morph.parse(token)[0].tag.POS))\n",
    "            # spacy find all собственные имена\n",
    "        return self.tagged\n",
    "\n",
    "    def text_dynamics(total_tags: list):\n",
    "        verbs = total_tags.count(\"VERB\")\n",
    "        return verbs / len(total_tags)\n",
    "\n",
    "    def text_dynamics_dict(self):\n",
    "        self.count_speech_parts()\n",
    "        verbs = self.tags_count[\"VERB\"]\n",
    "        total = 0\n",
    "        for i in self.tags_count.keys():\n",
    "            total += self.tags_count[i]\n",
    "        return verbs/total\n",
    "    \n",
    "    def count_speech_parts(self):\n",
    "        self.make_tags()\n",
    "        res = dict()\n",
    "        for token in self.tagged:\n",
    "            if (token[1] in res.keys()) :\n",
    "                res[token[1]] += 1\n",
    "            else:\n",
    "                res[token[1]] = 1\n",
    "        self.tags_count = res\n",
    "\n",
    "    def count_unique_words(self):\n",
    "        self.make_tags()\n",
    "        res = dict()\n",
    "        for token in self.tagged:\n",
    "            if not (token[1] in (\"CONJ\", \"PREP\", \"PRCL\")):\n",
    "                if token[0] in res.keys():\n",
    "                    res[token[0]] += 1\n",
    "                else:\n",
    "                    res[token[0]] = 1\n",
    "        self.word_count = res\n",
    "        self.word_count_sorted = dict(sorted(self.word_count.items(), key=lambda item: item[1]))\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_books(self, book1, book2):\n",
    "        text1 = self.get_book_text(book1)\n",
    "        text2 = self.get_book_text(book2)\n",
    "\n",
    "        doc1 = self.nlp(text1)\n",
    "        doc2 = self.nlp(text2)\n",
    "        return (doc1.similarity(doc2), np.dot(doc1.vector, doc2.vector) / (np.linalg.norm(doc1.vector) * (np.linalg.norm(doc2.vector))))\n",
    "\n",
    "    def count_root_speech_parts(self):\n",
    "        res = dict()\n",
    "        for token in self.doc:\n",
    "            if token.pos_ not in (\"SPACE\", \"PUNCT\"):\n",
    "                if token.pos_ not in res.keys():\n",
    "                    res[token.pos_] = 1\n",
    "                else:\n",
    "                    res[token.pos_] += 1\n",
    "        self.root_count = res\n",
    "        self.root_count_sorted = dict(sorted(self.root_count.items(), key=lambda item: item[1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text file reading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book(name):\n",
    "    with open(name, \"r\") as raw:\n",
    "        text = \"\"\n",
    "        for t in raw.readlines():\n",
    "            text += t+\"\\n\"\n",
    "        return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical diversity calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_diversity(word_list):\n",
    "    unique_words_set = set(word_list)\n",
    "    return len(unique_words_set) / len(word_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text dynamics calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_dynamics(total_tags: list):\n",
    "    verbs = total_tags.count(\"VERB\")\n",
    "    return verbs / len(total_tags)\n",
    "\n",
    "def text_dynamics_dict(tags_dict):\n",
    "    verbs = tags_dict[\"VERB\"]\n",
    "    total = 0\n",
    "    for i in tags_dict.keys():\n",
    "        total += tags_dict[i]\n",
    "    return verbs/total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morph analyzer init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def tagger(tokenized):\n",
    "    res = []\n",
    "    for token in tokenized:\n",
    "        res.append((token, morph.parse(token)[0].tag.POS))\n",
    "        # spacy find all собственные имена\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of speech counting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter to count speech parts\n",
    "def count_speech_parts(tagged):\n",
    "    res = dict()\n",
    "    for token in tagged:\n",
    "        if (token[1] in res.keys()) :\n",
    "            res[token[1]] += 1\n",
    "        else:\n",
    "            res[token[1]] = 1\n",
    "    return res\n",
    "\n",
    "def count_unique_words(tagged):\n",
    "    res = dict()\n",
    "    for token in tagged:\n",
    "        if not (token[1] in (\"CONJ\", \"PREP\", \"PRCL\")):\n",
    "            if token[0] in res.keys():\n",
    "                res[token[0]] += 1\n",
    "            else:\n",
    "                res[token[0]] = 1\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax analyzing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_root_speech_parts(doc):\n",
    "    res = dict()\n",
    "    for token in doc:\n",
    "        if token.pos_ not in (\"SPACE\", \"PUNCT\"):\n",
    "            if token.pos_ not in res.keys():\n",
    "                res[token.pos_] = 1\n",
    "            else:\n",
    "                res[token.pos_] += 1\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now onto the fun part\n",
    "# Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_books = [\"Похождения Чичикова (1921)\",\"Белая гвардия (1922)\",\"Роковые яйца (1924)\",\"Собачье сердце (1925)\",\"Мастер и маргарита (1929)\",\"Театральный роман (1936)\"]\n",
    "for name in list_of_books:\n",
    "    preproc = Preproc(name)\n",
    "    with open(name+\"_out\", \"w\") as file:\n",
    "        preproc.count_unique_words()\n",
    "        preproc.count_speech_parts()\n",
    "        preproc.count_root_speech_parts()\n",
    "        \n",
    "        file.write(f\"Lexical diversity: {str(preproc.lex_diversity_coeff)}\\n\")\n",
    "\n",
    "        file.write(\"Top-3 words: \\n\")\n",
    "        file.write(f\"\\t{list(preproc.word_count_sorted.keys())[-1]}\\n\")\n",
    "        file.write(f\"\\t{list(preproc.word_count_sorted.keys())[-2]}\\n\")\n",
    "        file.write(f\"\\t{list(preproc.word_count_sorted.keys())[-3]}\\n\")\n",
    "\n",
    "        file.write(f\"Text dynamics: {preproc.text_dynamics_dict()}\\n\")\n",
    "\n",
    "        file.write(f\"Top root speech part: {list(preproc.root_count_sorted.keys())[-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
