{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo list:\n",
    "- ~~book reading function~~\n",
    "- morphologic analysis:\n",
    "    - ~~lexical diversity calculation function~~\n",
    "    - ~~parts of speech counting function (by word's initial form)~~\n",
    "    - ~~parts of speech counting function (by tags, part of speech types)~~\n",
    "    - ~~text dynamics calculation functiion~~\n",
    "    - *top-1 for every speech part\n",
    "- syntax analysis\n",
    "    - reduce the amount of text\n",
    "    - ~~most popular root word~~\n",
    "- semantic analysis\n",
    "    - stop-word cleanup\n",
    "    - *automatic tag generation\n",
    "- graphic demonstration\n",
    "    - tag cloud"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/andrew/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pip install -q nltk pymorphy2\n",
    "import nltk\n",
    "import pymorphy2\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nlp = spacy.load('ru_core_news_sm')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a few constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT = ('.',',',':',';','\\'','\"','-','(',')','!','?','...','$','№')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preproc:\n",
    "    def __init__(self, book_name):\n",
    "        self.text = self.get_book_text(book_name)\n",
    "        self.tokens = nltk.word_tokenize(self.text)\n",
    "        self.morph = pymorphy2.MorphAnalyzer()\n",
    "        self.nlp = spacy.load('ru_core_news_sm')\n",
    "        self.make_tags()\n",
    "    \n",
    "    def process_stopwords(self, overwrite=False):\n",
    "        stopwords = stopwords.words(\"russian\")\n",
    "        if overwrite:\n",
    "            pass\n",
    "        else:\n",
    "            self.tokens_no_stopwords = [token for token in self.tokens if token not in stopwords]\n",
    "\n",
    "    def get_book_text(name):\n",
    "        with open(name, \"r\") as raw:\n",
    "            text = \"\"\n",
    "            for t in raw.readlines():\n",
    "                text += t+\"\\n\"\n",
    "            return text\n",
    "    \n",
    "    def lex_diversity(self):\n",
    "        unique_words_set = set(self.tokens)\n",
    "        return len(unique_words_set) / len(self.tokens)\n",
    "\n",
    "    def make_tags(self):\n",
    "        self.tagged = []\n",
    "        for token in self.tokens:\n",
    "            self.tagged.append((token, self.morph.parse(token)[0].tag.POS))\n",
    "            # spacy find all собственные имена\n",
    "        return self.tagged\n",
    "\n",
    "    def text_dynamics(total_tags: list):\n",
    "        verbs = total_tags.count(\"VERB\")\n",
    "        return verbs / len(total_tags)\n",
    "\n",
    "    def text_dynamics_dict(self):\n",
    "        self.count_speech_parts()\n",
    "        verbs = self.tags_count[\"VERB\"]\n",
    "        total = 0\n",
    "        for i in self.tags_count.keys():\n",
    "            total += self.tags_count[i]\n",
    "        return verbs/total\n",
    "    \n",
    "    def count_speech_parts(self):\n",
    "        self.make_tags()\n",
    "        res = dict()\n",
    "        for token in self.tagged:\n",
    "            if (token[1] in res.keys()) :\n",
    "                res[token[1]] += 1\n",
    "            else:\n",
    "                res[token[1]] = 1\n",
    "        self.tags_count = res\n",
    "\n",
    "    def count_unique_words(self):\n",
    "        self.make_tags()\n",
    "        res = dict()\n",
    "        for token in self.tagged:\n",
    "            if not (token[1] in (\"CONJ\", \"PREP\", \"PRCL\")):\n",
    "                if token[0] in res.keys():\n",
    "                    res[token[0]] += 1\n",
    "                else:\n",
    "                    res[token[0]] = 1\n",
    "        return res\n",
    "    \n",
    "    @staticmethod\n",
    "    def compare_books(self, book1, book2):\n",
    "        text1 = self.get_book_text(book1)\n",
    "        text2 = self.get_book_text(book2)\n",
    "\n",
    "        doc1 = self.nlp(text1)\n",
    "        doc2 = self.nlp(text2)\n",
    "        return (doc1.similarity(doc2), np.dot(doc1.vector, doc2.vector) / (np.linalg.norm(doc1.vector) * (np.linalg.norm(doc2.vector))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text file reading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book(name):\n",
    "    with open(name, \"r\") as raw:\n",
    "        text = \"\"\n",
    "        for t in raw.readlines():\n",
    "            text += t+\"\\n\"\n",
    "        return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical diversity calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_diversity(word_list):\n",
    "    unique_words_set = set(word_list)\n",
    "    return len(unique_words_set) / len(word_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text dynamics calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_dynamics(total_tags: list):\n",
    "    verbs = total_tags.count(\"VERB\")\n",
    "    return verbs / len(total_tags)\n",
    "\n",
    "def text_dynamics_dict(tags_dict):\n",
    "    verbs = tags_dict[\"VERB\"]\n",
    "    total = 0\n",
    "    for i in tags_dict.keys():\n",
    "        total += tags_dict[i]\n",
    "    return verbs/total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morph analyzer init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def tagger(tokenized):\n",
    "    res = []\n",
    "    for token in tokenized:\n",
    "        res.append((token, morph.parse(token)[0].tag.POS))\n",
    "        # spacy find all собственные имена\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of speech counting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# counter to count speech parts\n",
    "def count_speech_parts(tagged):\n",
    "    res = dict()\n",
    "    for token in tagged:\n",
    "        if (token[1] in res.keys()) :\n",
    "            res[token[1]] += 1\n",
    "        else:\n",
    "            res[token[1]] = 1\n",
    "    return res\n",
    "\n",
    "def count_unique_words(tagged):\n",
    "    res = dict()\n",
    "    for token in tagged:\n",
    "        if not (token[1] in (\"CONJ\", \"PREP\", \"PRCL\")):\n",
    "            if token[0] in res.keys():\n",
    "                res[token[0]] += 1\n",
    "            else:\n",
    "                res[token[0]] = 1\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax analyzing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_root_speech_parts(doc):\n",
    "    res = dict()\n",
    "    for token in doc:\n",
    "        if token.pos_ not in (\"SPACE\", \"PUNCT\"):\n",
    "            if token.pos_ not in res.keys():\n",
    "                res[token.pos_] = 1\n",
    "            else:\n",
    "                res[token.pos_] += 1\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now onto the fun part\n",
    "# Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_13607/16851473.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\t{list(words_count_sorted.keys())[-3]}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mspeech_part_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_speech_parts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_no_punct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Text dynamics: {text_dynamics_dict(speech_part_count)}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_13607/3165143899.py\u001b[0m in \u001b[0;36mtagger\u001b[0;34m(tokenized)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmorph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# spacy find all собственные имена\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_estimator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprob_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_to_parses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_lower\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36mapply_to_parses\u001b[0;34m(self, word, word_lower, parses)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         probs = [self.p_t_given_w.prob(word_lower, tag)\n\u001b[0m\u001b[1;32m     78\u001b[0m                 for (word, tag, normal_form, score, methods_stack) in parses]\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pymorphy2/analyzer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m         probs = [self.p_t_given_w.prob(word_lower, tag)\n\u001b[0m\u001b[1;32m     78\u001b[0m                 for (word, tag, normal_form, score, methods_stack) in parses]\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/pymorphy2/dawg.py\u001b[0m in \u001b[0;36mprob\u001b[0;34m(self, word, tag)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mdawg_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s:%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdawg_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMULTIPLIER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/dawg_python/dawgs.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \"\"\"\n\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLOOKUP_ERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "list_of_books = [\"Похождения Чичикова (1921)\",\"Белая гвардия (1922)\",\"Роковые яйца (1924)\",\"Собачье сердце (1925)\",\"Мастер и маргарита (1929)\",\"Театральный роман (1936)\"]\n",
    "for name in list_of_books:\n",
    "    text = get_book(name)\n",
    "    doc = nlp(text)\n",
    "\n",
    "    tokenized_no_punct = nltk.tokenize.RegexpTokenizer(r\"\\w+\").tokenize(text)\n",
    "    tagged = tagger(tokenized_no_punct)\n",
    "    with open(name+\"_out\", \"w\") as file:\n",
    "        file.write(f\"Lexical diversity: {str(lex_diversity(tokenized_no_punct))}\\n\")\n",
    "        file.write(\"Top-3 words: \\n\")\n",
    "        words_count = count_unique_words(tagger(tokenized_no_punct))\n",
    "        words_count_sorted = dict(sorted(words_count.items(), key=lambda item: item[1]))\n",
    "\n",
    "        file.write(f\"\\t{list(words_count_sorted.keys())[-1]}\\n\")\n",
    "        file.write(f\"\\t{list(words_count_sorted.keys())[-2]}\\n\")\n",
    "        file.write(f\"\\t{list(words_count_sorted.keys())[-3]}\\n\")\n",
    "\n",
    "        speech_part_count = count_speech_parts(tagger(tokenized_no_punct))\n",
    "        file.write(f\"Text dynamics: {text_dynamics_dict(speech_part_count)}\\n\")\n",
    "\n",
    "        root_count = count_root_speech_parts(doc)\n",
    "        root_count_sorted = dict(sorted(root_count.items(), key=lambda item: item[1]))\n",
    "        file.write(f\"Top root speech part: {list(root_count_sorted.keys())[-1]}\\n\")\n",
    "\n",
    "        # file.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
