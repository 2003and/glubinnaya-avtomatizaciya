{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo list:\n",
    "- ~~book reading function~~\n",
    "- ~~lexical diversity calculation function~~\n",
    "- parts of speech counting function (by word's initial form)\n",
    "- parts of speech counting function (by tags, part of speech types)\n",
    "- ~~text dynamics calculation functiion~~\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/andrew/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /home/andrew/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install -q nltk pymorphy2\n",
    "import nltk\n",
    "import pymorphy2\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a few constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCT = ('.',',',':',';','\\'','\"','-','(',')','!','?','...','$','№')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text file reading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_book(name):\n",
    "    with open(name, \"r\") as raw:\n",
    "        text = \"\"\n",
    "        for t in raw.readlines():\n",
    "            text += t+\"\\n\"\n",
    "        return text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical diversity calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lex_diversity(word_list):\n",
    "    unique_words_set = set(word_list)\n",
    "    return len(unique_words_set) / len(word_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text dynamics calculation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_dynamics(total_tags: list):\n",
    "    verbs = total_tags.count(\"VERB\")\n",
    "    return verbs / len(total_tags)\n",
    "\n",
    "def text_dynamics_dict(tags_dict):\n",
    "    verbs = tags_dict[\"VERB\"]\n",
    "    total = 0\n",
    "    for i in tags_dict.keys():\n",
    "        total += tags_dict[i]\n",
    "    return verbs/total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Morph analyzer init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "def tagger(tokenized):\n",
    "    res = []\n",
    "    for token in tokenized:\n",
    "        res.append((token, morph.parse(token)[0].tag.POS))\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of speech counting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_speech_parts(tagged):\n",
    "    res = dict()\n",
    "    for token in tagged:\n",
    "        if token[1] in res.keys():\n",
    "            res[token[1]] += 1\n",
    "        else:\n",
    "            res[token[1]] = 1\n",
    "\n",
    "    return res\n",
    "\n",
    "def count_unique_words(tokenized):\n",
    "    res = dict()\n",
    "    for token in tokenized:\n",
    "        if token in res.keys():\n",
    "            res[token] += 1\n",
    "        else:\n",
    "            res[token] = 1\n",
    "\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntax analyzing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now onto the fun part\n",
    "# Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_books = [\"Собачье сердце (фрагмент)\",]\n",
    "for name in list_of_books:\n",
    "    text = get_book(name)\n",
    "    tokenized_no_punct = nltk.tokenize.RegexpTokenizer(r\"\\w+\").tokenize(text)\n",
    "    tagged = tagger(tokenized_no_punct)\n",
    "    with open(name+\"_out\", \"w\") as file:\n",
    "        file.write(f\"Lexical diversity: {str(lex_diversity(tokenized_no_punct))}\\n\")\n",
    "        file.write(\"Top-3 words: \\n\")\n",
    "        words_count = count_unique_words(tokenized_no_punct)\n",
    "        words_count_sorted = dict(sorted(words_count.items(), key=lambda item: item[1]))\n",
    "        \n",
    "        file.write(f\"\\t{list(words_count_sorted.keys())[-1]}\\n\")\n",
    "        file.write(f\"\\t{list(words_count_sorted.keys())[-2]}\\n\")\n",
    "        file.write(f\"\\t{list(words_count_sorted.keys())[-3]}\\n\")\n",
    "\n",
    "        speech_part_count = count_speech_parts(tagger(tokenized_no_punct))\n",
    "        file.write(f\"Text dynamics: {text_dynamics_dict(speech_part_count)}\")\n",
    "        # file.write(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
